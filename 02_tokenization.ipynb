{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0977200d-4c83-4e62-a68a-eb3095ed5961",
   "metadata": {},
   "source": [
    "# Tokenization in NLP\n",
    "\n",
    "## Introduction\n",
    "Tokenization is the process of breaking text into smaller units called **tokens**. Tokens can be:\n",
    "- Words\n",
    "- Sentences\n",
    "- Subwords (used in modern models like BERT/GPT)\n",
    "\n",
    "Tokenization is a **crucial step** in NLP because it transforms raw text into a structured format that models can understand.\n",
    "\n",
    "**In this notebook, we will learn:**\n",
    "1. Word tokenization\n",
    "2. Sentence tokenization\n",
    "3. Subword tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32604be7-18c2-44f1-a417-51a3fc5b26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Hello there! NLP is amazing. Let's learn tokenization.\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import libraries\n",
    "import re  # for regex sentence splitting\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello there! NLP is amazing. Let's learn tokenization.\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76bea32-cd44-41d5-b73f-80b928e72f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello', 'there!', 'NLP', 'is', 'amazing.', \"Let's\", 'learn', 'tokenization.']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Word Tokenization (Basic)\n",
    "# Using simple split\n",
    "words = text.split()\n",
    "print(\"Word Tokens:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277fdb3-903f-40e6-a208-6f7094309413",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- Word tokenization splits the text into individual words.  \n",
    "- Here we used `.split()` for simplicity.  \n",
    "- This method works on all systems without any extra library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91642203-9bbd-4892-b14e-beca7438f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Hello there!', 'NLP is amazing.', \"Let's learn tokenization.\"]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sentence Tokenization (Basic)\n",
    "# Using regex to split by punctuation\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "print(\"Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd98fe-308f-49aa-81d8-fa8835760e0e",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- Sentence tokenization splits the text into sentences.  \n",
    "- We used regex to split sentences by `.`, `!`, or `?`.  \n",
    "- This is a simple and reliable way without NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e59e09a-66d3-4581-b007-21f016cb5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello there! How are you?\n",
      "Word Tokens: ['Hello', 'there!', 'How', 'are', 'you?']\n",
      "--------------------------------------------------\n",
      "Original: Tokenization is essential for NLP.\n",
      "Word Tokens: ['Tokenization', 'is', 'essential', 'for', 'NLP.']\n",
      "--------------------------------------------------\n",
      "Original: We will learn preprocessing step by step.\n",
      "Word Tokens: ['We', 'will', 'learn', 'preprocessing', 'step', 'by', 'step.']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenizing a small dataset\n",
    "sample_texts = [\n",
    "    \"Hello there! How are you?\",\n",
    "    \"Tokenization is essential for NLP.\",\n",
    "    \"We will learn preprocessing step by step.\"\n",
    "]\n",
    "\n",
    "# Word tokenization for all sentences\n",
    "tokenized_texts = [text.split() for text in sample_texts]\n",
    "\n",
    "for i, tokens in enumerate(tokenized_texts):\n",
    "    print(f\"Original: {sample_texts[i]}\")\n",
    "    print(f\"Word Tokens: {tokens}\")\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1188e404-458a-4a24-acd7-feb339c30901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword Tokens (Basic): [['He', 'llo'], ['the', 're!'], ['N', 'LP'], ['i', 's'], ['amaz', 'ing.'], ['Le', \"t's\"], ['le', 'arn'], ['tokeni', 'zation.']]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Subword Tokenization\n",
    "# Split words into smaller units using basic rules\n",
    "\n",
    "def simple_subword_tokenize(word):\n",
    "    # Example: split words into halves or syllable-like chunks\n",
    "    mid = len(word) // 2\n",
    "    if mid == 0:\n",
    "        return [word]\n",
    "    return [word[:mid], word[mid:]]\n",
    "\n",
    "subword_tokens = [simple_subword_tokenize(w) for w in words]\n",
    "print(\"Subword Tokens (Basic):\", subword_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c462d2-acd1-4db7-b4be-bf9061adff95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
